---
last_updated: '1 May 2025'
bugs:
  - name: PG Splitting/Merging Causes OSD Out-Of-Memory
    description: A bug in the PG splitting and merging code can cause the OSD to go out-of-memory, a condition which persists even after restart. Offline tools are available in fixed releases to workaround the issue. See https://tracker.ceph.com/issues/53729.
    recommendation: Do not change pg_num for any pool until after upgrade to a fixed release. Disable the pg autoscaler. Fixed in v16.2.11 and v17.2.4.
    severity: high
    affected_versions:
      - 15.2.*
      - 16.2.[0-10]
      - 17.2.[0-3]
  - name: Pacific Broken Hotfixes
    description: Version 16.2.11 has a bug in ceph-volume osd activation, which was meant to be hotfixed in 16.2.12 but that release was built incorrectly, including changes which did not pass the rigorous QA testing.
    severity: high
    recommendation: It is recommended to upgrade to version 16.2.13 or later.
    affected_versions:
      - 16.2.[11-12]
  - name: Squid deployed OSDs are crashing
    description: The issue affects only newly deployed OSDs using Squid, while previously deployed OSDs run fine. It is likely caused by the Elastic Shared Blob implementation introduced in this PR https://github.com/ceph/ceph/pull/53178, and ceph-bluestore-tool repair cannot fix it.
    severity: critical
    recommendation: For now, run "ceph config set osd bluestore_elastic_shared_blobs 0" on any Squid cluster before adding new OSDs. Unfortunately, this will not help OSDs already deployed in Squidâ€”the only known fix for them is redeployment.
    affected_versions:
      - 19.*.*
  - name: BlueStore Potential Corruption
    description: Some versions of Ceph were released with a bug that may cause OSDs to crash and corrupt the on-disk data. Upstream ticket https://tracker.ceph.com/issues/69764
    severity: critical
    recommendation: Upgrade to a fixed version (17.2.9 or 18.2.7) as soon as possible.
    affected_versions:
      - 17.2.8
      - 18.2.5
      - 18.2.6
  - name: "v19.2.1 bdev_async_discard incompatibility and CPU usage"
    description: "Ceph v19.2.1 replaced bdev_async_discard with bdev_async_discard_thread (https://github.com/ceph/ceph/pull/59065), breaking backward compatibility. Old settings are ignored, disabling async discards. New settings via automation (e.g., Rook) can fail. Setting bdev_async_discard_thread > 1 causes 100% OSD CPU usage. Upstream tracker: https://tracker.ceph.com/issues/70327"
    recommendation: "Upgrade to Ceph v19.2.3 or later where this is resolved. A PR (https://github.com/ceph/ceph/pull/62254) was also made to reintroduce the bdev_async_discard parameter."
    severity: "high"
    affected_versions:
      - "19.2.1"
